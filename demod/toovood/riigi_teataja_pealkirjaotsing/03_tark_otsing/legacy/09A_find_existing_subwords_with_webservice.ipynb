{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c823161b-4457-4a17-ae8f-df7f607a1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import urllib.parse\n",
    "\n",
    "from datetime import date\n",
    "from pandas import DataFrame \n",
    "from pandas import read_csv\n",
    "from pandas import concat\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d070a-14f8-4b78-ac5d-d1f8a925bb9f",
   "metadata": {},
   "source": [
    "## I. Using vabamorph analyser directly to extract wordform-lemma pairs from text\n",
    "\n",
    "This part of the code is exactly the sama as for lemma exraction (05A_find_existing_lemmas_in_document_captions_with_webservice.ipynb).\n",
    "* We use the updated version of the script here.\n",
    "* Updated the treatment of quotation marks.\n",
    "* Updated the treatment of brakets.\n",
    "* Updated treatment of comas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e17927a-3d14-45a8-ae0b-4bb7b6ee0578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>wordform</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sublemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Presidendi</td>\n",
       "      <td>president</td>\n",
       "      <td>[president]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ametiraha</td>\n",
       "      <td>ametiraha</td>\n",
       "      <td>[ameti, raha]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>seadus</td>\n",
       "      <td>seadus</td>\n",
       "      <td>[seadus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>seadus</td>\n",
       "      <td>seadu</td>\n",
       "      <td>[seadu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    wordform      lemma      sublemmas\n",
       "0      0  Presidendi  president    [president]\n",
       "1      1   ametiraha  ametiraha  [ameti, raha]\n",
       "2      2      seadus     seadus       [seadus]\n",
       "2      2      seadus      seadu        [seadu]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_document_caption(caption: str):\n",
    "    \"\"\"\n",
    "    Uses web service to extract words and sub-words form document captions\n",
    "\n",
    "    Returns a four column table with columns index, wordform, lemma, sublemmas.\n",
    "    There can be several rows for each word as each word is analysed separately.\n",
    "    All rows with the same index correspond to the same word.\n",
    "    Wordform columns is added to facilitate tokenisation debugging.\n",
    "    \"\"\"\n",
    "    corrected_caption = re.sub('\\s+', ' ', re.sub('ˮ|\"|„|”|“|«|»', ' ˮ ', re.sub('\\s+', ' ', caption)))\n",
    "    corrected_caption = re.sub('\\s+', ' ', re.sub('\\)', ' )', re.sub('\\(', '( ', corrected_caption)))\n",
    "    corrected_caption = re.sub('\\s+', ' ', re.sub('\\.(?:\\s|$)', ' . ', re.sub(',(?:\\s|$)', ' , ', corrected_caption)))\n",
    "    corrected_caption = re.sub('\\s+', ' ', re.sub(':(?:\\s|$)', ' : ', re.sub(';(?:\\s|$)', ' ; ', corrected_caption)))\n",
    "\n",
    "    ANALYZER_QUERY = \"https://smart-search.tartunlp.ai/api/analyser/process\"\n",
    "    HEADERS = {\"Content-Type\": \"application/json; charset=utf-8\"}\n",
    "    POST_DATA_TEMPLATE = {'params': {\"vmetajson\": [\"--guess\"]}, 'content': corrected_caption}\n",
    "\n",
    "    response = requests.post(ANALYZER_QUERY, json=POST_DATA_TEMPLATE, headers=HEADERS)\n",
    "    assert response.ok, \"Webservice failed\"\n",
    "    response = response.json()\n",
    " \n",
    "    token_count = len(response['annotations']['tokens'])\n",
    "    tbl = DataFrame({'wordform': [None] * token_count, 'lemma': [None] * token_count})\n",
    "    for i, token in enumerate(response['annotations']['tokens']):\n",
    "        features = token['features']\n",
    "        tbl.loc[i, 'wordform'] = features['token']\n",
    "        tbl.loc[i, 'lemma'] = list(set(map(lambda x: x['lemma'], features['mrf'])))\n",
    "\n",
    "    tbl =  tbl.reset_index().explode('lemma')\n",
    "\n",
    "    # Post-correction for Vabamorph output. Remove special symbols \n",
    "    tbl['lemma'] = tbl['lemma'].str.replace('=', '', regex=False)\n",
    "    tbl['lemma'] = tbl['lemma'].str.replace('+', '', regex=False)\n",
    "\n",
    "    # Post-correction for sublemmas\n",
    "    tbl['sublemmas'] = tbl['lemma'].str.split('_', regex=False)\n",
    "    tbl['lemma'] = tbl['lemma'].str.replace('_', '', regex=False)\n",
    "    return tbl\n",
    "\n",
    "# „Talleks”\t\n",
    "# '\"Balti'\n",
    "# '“Lepingu'\n",
    "# (EAEC)\t\n",
    "# Ühendust,\n",
    "# «Õigusabi\n",
    "# Example output\n",
    "analyze_document_caption('Presidendi ametiraha seadus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364808b-c1d1-4b86-93b7-3ed51bfeba45",
   "metadata": {},
   "source": [
    "## I.A Initial analysis of all captions\n",
    "\n",
    "* We need to analyse all the document captions and get occurence counts for wordforms.\n",
    "* In this analysis we ignore the fact that some wordforms have multiple lemmas and treat each row with the same weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7509980c-7602-4ff8-bd46-8c27b1134484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a4b30746814c318f6ff81188a9d29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>wordform</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sublemmas</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ajutise</td>\n",
       "      <td>ajutine</td>\n",
       "      <td>[ajutine]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sisseveo</td>\n",
       "      <td>sissevedu</td>\n",
       "      <td>[sisse, vedu]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>konventsiooniga</td>\n",
       "      <td>konventsioon</td>\n",
       "      <td>[konventsioon]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ühinemise</td>\n",
       "      <td>ühinemine</td>\n",
       "      <td>[ühinemine]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>seadus</td>\n",
       "      <td>seadus</td>\n",
       "      <td>[seadus]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Väljasõidukohustuse</td>\n",
       "      <td>väljasõidukohustus</td>\n",
       "      <td>[välja, sõidu, kohustus]</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ja</td>\n",
       "      <td>ja</td>\n",
       "      <td>[ja]</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sissesõidukeelu</td>\n",
       "      <td>sissesõidukeeld</td>\n",
       "      <td>[sisse, sõidu, keeld]</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>seadus</td>\n",
       "      <td>seadus</td>\n",
       "      <td>[seadus]</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>seadus</td>\n",
       "      <td>seadu</td>\n",
       "      <td>[seadu]</td>\n",
       "      <td>4711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24339 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index             wordform               lemma                 sublemmas  \\\n",
       "0       0              Ajutise             ajutine                 [ajutine]   \n",
       "1       1             sisseveo           sissevedu             [sisse, vedu]   \n",
       "2       2      konventsiooniga        konventsioon            [konventsioon]   \n",
       "3       3            ühinemise           ühinemine               [ühinemine]   \n",
       "4       4               seadus              seadus                  [seadus]   \n",
       "..    ...                  ...                 ...                       ...   \n",
       "0       0  Väljasõidukohustuse  väljasõidukohustus  [välja, sõidu, kohustus]   \n",
       "1       1                   ja                  ja                      [ja]   \n",
       "2       2      sissesõidukeelu     sissesõidukeeld     [sisse, sõidu, keeld]   \n",
       "3       3               seadus              seadus                  [seadus]   \n",
       "3       3               seadus               seadu                   [seadu]   \n",
       "\n",
       "    doc_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "0     4711  \n",
       "1     4711  \n",
       "2     4711  \n",
       "3     4711  \n",
       "3     4711  \n",
       "\n",
       "[24339 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sources = read_csv('results/state_laws.csv', header=0)\n",
    "\n",
    "result = [None] *  len(sources)\n",
    "for i, caption in tqdm(enumerate(sources['document_title']), total=len(sources)):\n",
    "    result[i] = analyze_document_caption(caption).assign(doc_id = i)\n",
    "\n",
    "result = concat(result, axis=0).reset_index(drop=True)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d493253-f21a-4d7d-9fcc-19b95b233b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f073e-84db-4f32-8b7a-0de9829a3d42",
   "metadata": {},
   "source": [
    "### Explicit decomposition into subwords\n",
    "\n",
    "Vabamorf analyser returns lemma of a compound word together with its decomposition into subwords. \n",
    "All compounds except for the last subword are in the form they occur in the original wordform.\n",
    "Thus it is straightforward although tedious to extract all subwords,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81ed123a-f201-4bd8-becd-c9d25e5e424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_words = result[result['sublemmas'].map(lambda x: len(x) != 1)].copy()\n",
    "compound_words['prefix'] = compound_words['sublemmas'].map(lambda x: ''.join(x[:len(x)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3699179-0867-4c91-949a-02d1e379a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suffic(word: str, prefix: str):\n",
    "    location = word.lower().find(prefix.lower())\n",
    "    return word[location + len(prefix):] if location != -1 else None    \n",
    "\n",
    "assert extract_suffic('subword', 'sub') == 'word'\n",
    "assert extract_suffic('-subword', 'sub') == 'word'\n",
    "assert extract_suffic('subword', 'xxx') is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1faaeee1-4efb-4a86-ae59-0d591b473982",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_words['subwordforms'] = compound_words.apply(\n",
    "    lambda row: row['sublemmas'][:-1] + [extract_suffic(row['wordform'], row['prefix'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61348803-9818-468c-8c19-244ea408739f",
   "metadata": {},
   "source": [
    "### Validation of compound word table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4b2166f6-ee20-4932-8d61-c6cc86a212f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESTONIAN_LETTER = '[a-z|öäõü|\\-|žš]'\n",
    "ESTONIAN_ONLY_WORD = f'^{ESTONIAN_LETTER}+$'\n",
    "\n",
    "idx = compound_words['wordform'].str.match(ESTONIAN_ONLY_WORD, case=False)\n",
    "assert all(idx), 'Selected compound words do not contain anything unexpected'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca20c8-1e2c-4719-b4a9-962e71003ecf",
   "metadata": {},
   "source": [
    "### Export results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "65a375c7-e52d-41dc-930f-fadb074eb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = (compound_words[['index', 'doc_id', 'subwordforms']]\n",
    "            .explode('subwordforms')\n",
    "            .rename(columns={'subwordforms':'subword'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7334365b-441c-4ef7-b3fe-28946929e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (subwords\n",
    "          .groupby('subword', as_index=False)\n",
    "          .agg(occurence_count=('index', lambda x: len(x)), document_count=('doc_id', lambda x: len(set(x))))\n",
    "          .sort_values(['occurence_count', 'document_count'], ascending=False))\n",
    "\n",
    "output.to_csv('results/caption_index/state_laws_existing_subwords.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
